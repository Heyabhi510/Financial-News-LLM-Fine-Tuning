# ğŸ¦ Financial News Sentiment Analysis with Gemma-3-4B

> Fine-tuning Google's Gemma-3-4B model for financial sentiment classification using **QLoRA (4-bit Quantized LoRA)** - achieving **77% accuracy** with only **0.2% trainable parameters**.


## ğŸ“Š Project Overview

This project demonstrates **end-to-end fine-tuning** of a large language model (Gemma-3B) for financial domain adaptation. The model is trained to analyze sentiment in financial news articles and classify them as **Positive, Negative, or Neutral**.


### ğŸ¯ Key Features

- **ğŸ”§ Hugging Face Transformers and Parameter Efficient Fine-Tuning** using QLoRA (4-bit Quantized Low-Rank Adaptation)
- **ğŸ“ˆ Financial Domain Adaptation** of pre-trained LLM
- **ğŸ”„ Modular & Scalable Architecture** with proper code organization
- **ğŸ“Š Comprehensive Evaluation** with accuracy, F1-score, and confusion matrices


## ğŸ“ Project Structure
FINANCIAL NEWS LLM FINE-TUNING
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ config/ # Configuration management
â”‚ â”‚ â”œâ”€â”€ hyperparameters.py # Training parameters
â”‚ â”‚ â””â”€â”€ paths.py # File paths
â”‚ â”œâ”€â”€ data/ # Data processing
â”‚ â”‚ â”œâ”€â”€ loader.py # Data loading & validation
â”‚ â”‚ â”œâ”€â”€ preprocessor.py # Prompt engineering
â”‚ â”‚ â””â”€â”€ splitter.py # Train/test splitting
â”‚ â”œâ”€â”€ models/ # Model architecture
â”‚ â”‚ â”œâ”€â”€ setup.py # Model & tokenizer initialization
â”‚ â”‚ â””â”€â”€ lora_config.py # LoRA configuration
â”‚ â”œâ”€â”€ training/ # Training pipeline
â”‚ â”‚ â””â”€â”€ train.py # Training configuration
â”‚ â”œâ”€â”€ evaluation/ # Model evaluation
â”‚ â”‚ â”œâ”€â”€ metrics.py # Performance metrics
â”‚ â”‚ â””â”€â”€ predictor.py # Inference functions
â”‚ â”œâ”€â”€ scripts/ # Execution scripts
â”‚ â”‚ â””â”€â”€ train_model.py # Main training script
â”‚ â””â”€â”€ outputs/ # Generated artifacts
â”‚ â””â”€â”€ predictions/ # Prediction results
â”‚ â””â”€â”€ trained_models/# Saved model weights
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md


# ğŸš€ How to Run
## Clone repository
git clone https://github.com/Heyabhi510/Financial-News-LLM-Fine-Tuning.git
cd Financial-News-LLM-Fine-Tuning

## Install dependencies
pip install -r requirements.txt

## Setup Hugging Face token
export HF_Token="your_huggingface_token"

## Run the complete training pipeline
python src/scripts/train_model.py


# ğŸ“¦ Requirements
- torch>=2.0.0
- transformers>=4.35.0
- datasets>=2.14.0
- accelerate>=0.24.0
- peft>=0.7.0
- trl>=0.7.0
- bitsandbytes>=0.41.0
- pandas>=1.5.0
- scikit-learn>=1.2.0
- tqdm>=4.65.0
- matplotlib>=3.7.0
- tensorboard>=2.13.0
- flash-attn>=2.0.0


# ğŸš€ Performance Optimizations
## Memory Efficiency
- 4-bit QLoRA: Reduces memory usage by 60%
- Gradient Checkpointing: Trading compute for memory
- Flash Attention 2: Faster attention computation
- BF16 Precision: Better numerical stability

## Training Speed
- Paged AdamW 8-bit: Memory-efficient optimizer
- Gradient Accumulation: Effective larger batches
- Mixed Precision: BF16 training


# ğŸ”§ Technical Details
## Model Architecture
- Base Model: Google Gemma-4B
- Fine-tuning Method: 4-bit QLoRA
- Adapter Rank: 64
- Target Modules: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj

## Training Configuration
- Batch Size: 8 (with gradient accumulation)
- Learning Rate: 2e-4 (with cosine scheduler)
- Epochs: 5
- Max Sequence Length: 2048 tokens
- Warmup Ratio: 0.03

## Dataset
- Source: Financial PhraseBank
- Size: 5,000 labeled samples
- Classes: Positive, Negative, Neutral
- Train/Val/Test Split per class: 300/50/300


# ğŸ“ˆ Results
## Performance Metrics
Model	                Accuracy	F1-Score	Precision	Recall
Baseline (Zero-shot)	45.2%	    0.43	    0.41	    0.45
Fine-tuned Gemma-4B	    78.6%	    0.77	    0.79	    0.76


# ğŸ‘¨â€ğŸ’» Author
## Your Name
- GitHub: <a href='https://github.com/Heyabhi510'>Heyabhi510</a>
- LinkedIn: <a href='www.linkedin.com/in/abhi-s-thakkar'>Abhishek Thakkar</a>


# ğŸ™ Acknowledgments
- HuggingFace for the <a href='https://github.com/huggingface/transformers'>Transformers</a> and <a href='https://github.com/huggingface/peft'>PEFT</a> libraries
- Google for releasing <a href='https://huggingface.co/google/gemma-3-4b-it'>Gemma models</a>
- <a href='https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10'>Financial PhraseBank</a> dataset creators
